{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [spaCy overview](http://spacy.io/docs/#examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spaCy resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import spacy and English models\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Here are two sentences.' using spaCy\n",
    "\n",
    "doc = nlp('Hello, world. Here are two sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tokens and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "print()\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual part of speech tagging ([displaCy](https://displacy.spacy.io))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a function that walk up the syntactic tree of the given token and collects all tokent to the root token (including root token).\n",
    "def tokens_to_root(token):\n",
    "    '''Walk up the syntactic tree, collecting tokens to the root.'''\n",
    "    tokens_to_r = []\n",
    "    while token.head is not token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        \n",
    "    tokens_to_r.append(token)\n",
    "    return tokens_to_r\n",
    "\n",
    "# For every token in document, print it's tokens to the root\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "    \n",
    "print()\n",
    "# Print dependency labels of the tokens\n",
    "for token in doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print all named entities with named entity types\n",
    "\n",
    "doc_2 = nlp(\"I went to Paris where I met my old friend Jack from uni.\")\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print noun chunks for doc_2\n",
    "\n",
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For every token in doc_2, print log-probability of the word, estimated from counts from a large corpus \n",
    "\n",
    "for token in doc_2:\n",
    "    print(token, ',', token.prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding / Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a given document, caclulate similarity between 'apples' and 'oranges' and 'boots' amd 'hippos'\n",
    "doc = nlp(\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print(apples.similarity(oranges))\n",
    "print(boots.similarity(hippos))\n",
    "\n",
    "print()\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab['fruit']\n",
    "print(apples_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(fruit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matplotlib Jupyter HACK\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Process full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process `text` with Spacy NLP Parser\n",
    "text = read_file('data/pride_and_prejudice.txt')\n",
    "processed_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many sentences are in Pride & Prejudice book?\n",
    "sentences = [s for s in processed_text.sents]\n",
    "print(len(sentences))\n",
    "\n",
    "# Print sentences from index 10 to index 15, to make sure that we have parsed correct book\n",
    "print(sentences[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all the personal names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract all the personal names from Pride & Prejudice and count theirs occurences. \n",
    "# Expected output is a list in the following form: [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266) ...].\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "actors = Counter()\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        actors[ent.lemma_] += 1\n",
    "        \n",
    "print(actors.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot actors personal names as a time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot actor mentions as a time series relative to the position of the actor's ocurence in a book.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "actors_occurences = defaultdict(list)\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        actors_occurences[ent.lemma_].append(ent.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import hist\n",
    "\n",
    "NUM_BINS = 10\n",
    "\n",
    "def normalize_occurences(occurencies):\n",
    "    return [o / float(len(processed_text)) for o in occurencies]\n",
    "\n",
    "elizabeth_occurences = normalize_occurences(actors_occurences['elizabeth'])\n",
    "darcy_occurences = normalize_occurences(actors_occurences['darcy'])\n",
    "bingly_occurences = normalize_occurences(actors_occurences['bingley'])\n",
    "\n",
    "x = [elizabeth_occurences, darcy_occurences, bingly_occurences,]\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    n, bins, patches = plt.hist(x, NUM_BINS, histtype='bar', label=['Elizabeth', 'Darcy', 'Bingley'])\n",
    "    plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('fivethirtyeight'):\n",
    "    for a in n:\n",
    "        plt.plot([x / (NUM_BINS - 1) for x in range(len(a))], a)\n",
    "\n",
    "    plt.legend(['elizabeth', 'darcy', 'bingley'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy parse tree in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find words (adjectives) that describe Mr Darcy. \n",
    "\n",
    "# Solution #1\n",
    "darcy_adjectives = []\n",
    "for ent in processed_text.ents:\n",
    "    if ent.lemma_ == 'darcy':\n",
    "        for token in ent.subtree:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                darcy_adjectives.append(token.lemma_)\n",
    "print(darcy_adjectives)\n",
    "\n",
    "print()\n",
    "# Solution #2\n",
    "# Definition of Adjectival modifier http://universaldependencies.org/en/dep/amod.html\n",
    "print([token.lemma_ for ent in processed_text.ents if ent.lemma_ == 'darcy' for token in ent.subtree if token.dep_ == 'amod'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find actors that are 'talking', 'saying', 'doing' the most. Find the relationship between \n",
    "# entities and corresponding root verbs.\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "print(Counter([ent.lemma_ for ent in processed_text.ents if ent.label_ == 'PERSON' and ent.root.head.lemma_=='say']).most_common()) \n",
    "\n",
    "\n",
    "print()\n",
    "# Find all the actors that got married in the book\n",
    "\n",
    "# Some sentence from which information could be extracted\n",
    "# \n",
    "# her mother was talking to that one person (Lady Lucas) freely,\n",
    "# openly, and of nothing else but her expectation that Jane would soon\n",
    "# be married to Mr. Bingley.\n",
    "#\n",
    "print(Counter([ent.lemma_ for ent in processed_text.ents if ent.label_ == 'PERSON' and ent.root.head.lemma_=='marry']).most_common()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Keywords using noun chunks from the news article (file 'article.txt').\n",
    "# Spacy will pick some noun chunks that are not informative at all (e.g. we, what, who).\n",
    "# Try to find a way to remove that kind of keywords.\n",
    "\n",
    "article = read_file('data/article.txt')\n",
    "doc = nlp(article)\n",
    "\n",
    "keywords = Counter()\n",
    "for chunk in doc.noun_chunks:\n",
    "    if nlp.vocab[chunk.lemma_].prob < - 8: # probablity value -8 is arbitraraly selected threshold\n",
    "        keywords[chunk.lemma_] += 1\n",
    "\n",
    "keywords.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
