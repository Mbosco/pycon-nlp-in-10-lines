{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [spaCy overview](http://spacy.io/docs/#examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spaCy resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import spacy and English models\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Here are two sentences.' using spaCy\n",
    "\n",
    "doc = nlp('Hello, world. Here are two sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tokens and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "print()\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual part of speech tagging ([displaCy](https://displacy.spacy.io))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a function that walk up the syntactic tree of the given token and collects all tokens to the root token (including root token).\n",
    "def tokens_to_root(token):\n",
    "    '''Walk up the syntactic tree, collecting tokens to the root.'''\n",
    "    tokens_to_r = []\n",
    "    while token.head is not token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        \n",
    "    tokens_to_r.append(token)\n",
    "    return tokens_to_r\n",
    "\n",
    "# For every token in document, print it's tokens to the root\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "    \n",
    "print()\n",
    "# Print dependency labels of the tokens\n",
    "for token in doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print all named entities with named entity types\n",
    "\n",
    "doc_2 = nlp(\"I went to Paris where I met my old friend Jack from uni.\")\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print noun chunks for doc_2\n",
    "\n",
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For every token in doc_2, print log-probability of the word, estimated from counts from a large corpus \n",
    "\n",
    "for token in doc_2:\n",
    "    print(token, ',', token.prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding / Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For a given document, caclulate similarity between 'apples' and 'oranges' and 'boots' amd 'hippos'\n",
    "doc = nlp(\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print(apples.similarity(oranges))\n",
    "print(boots.similarity(hippos))\n",
    "\n",
    "print()\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab['fruit']\n",
    "print(apples_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(fruit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matplotlib Jupyter HACK\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Process full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process `text` with Spacy NLP Parser\n",
    "text = read_file('data/pride_and_prejudice.txt')\n",
    "processed_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many sentences are in Pride & Prejudice book?\n",
    "sentences = [s for s in processed_text.sents]\n",
    "print(len(sentences))\n",
    "\n",
    "# Print sentences from index 10 to index 15, to make sure that we have parsed correct book\n",
    "print(sentences[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all the personal names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract all the personal names from Pride & Prejudice and count theirs occurences. \n",
    "# Expected output is a list in the following form: [('elizabeth', 622), ('darcy', 312), ('jane', 286), ('bennet', 266) ...].\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "actors = Counter()\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        actors[ent.lemma_] += 1\n",
    "        \n",
    "print(actors.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot actors personal names as a time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot actor mentions as a time series relative to the position of the actor's occurence in a book.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "actors_occurences = defaultdict(list)\n",
    "for ent in processed_text.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        actors_occurences[ent.lemma_].append(ent.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import hist\n",
    "\n",
    "NUM_BINS = 10\n",
    "\n",
    "def normalize_occurences(occurencies):\n",
    "    return [o / float(len(processed_text)) for o in occurencies]\n",
    "\n",
    "elizabeth_occurences = normalize_occurences(actors_occurences['elizabeth'])\n",
    "darcy_occurences = normalize_occurences(actors_occurences['darcy'])\n",
    "bingly_occurences = normalize_occurences(actors_occurences['bingley'])\n",
    "\n",
    "x = [elizabeth_occurences, darcy_occurences, bingly_occurences,]\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    n, bins, patches = plt.hist(x, NUM_BINS, histtype='bar', label=['Elizabeth', 'Darcy', 'Bingley'])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('fivethirtyeight'):\n",
    "    for a in n:\n",
    "        plt.plot([x / (NUM_BINS - 1) for x in range(len(a))], a)\n",
    "\n",
    "    plt.legend(['elizabeth', 'darcy', 'bingley'], bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy parse tree in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find words (adjectives) that describe Mr Darcy. \n",
    "\n",
    "# Solution #1\n",
    "darcy_adjectives = []\n",
    "for ent in processed_text.ents:\n",
    "    if ent.lemma_ == 'darcy':\n",
    "        for token in ent.subtree:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                darcy_adjectives.append(token.lemma_)\n",
    "print(darcy_adjectives)\n",
    "\n",
    "print()\n",
    "# Solution #2\n",
    "# Definition of Adjectival modifier http://universaldependencies.org/en/dep/amod.html\n",
    "print([token.lemma_ for ent in processed_text.ents if ent.lemma_ == 'darcy' for token in ent.subtree if token.dep_ == 'amod'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find actors that are 'talking', 'saying', 'doing' the most. Find the relationship between \n",
    "# entities and corresponding root verbs.\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "print(Counter([ent.lemma_ for ent in processed_text.ents if ent.label_ == 'PERSON' and ent.root.head.lemma_=='say']).most_common()) \n",
    "\n",
    "\n",
    "print()\n",
    "# Find all the actors that got married in the book\n",
    "\n",
    "# Some sentence from which information could be extracted\n",
    "# \n",
    "# her mother was talking to that one person (Lady Lucas) freely,\n",
    "# openly, and of nothing else but her expectation that Jane would soon\n",
    "# be married to Mr. Bingley.\n",
    "#\n",
    "print(Counter([ent.lemma_ for ent in processed_text.ents if ent.label_ == 'PERSON' and ent.root.head.lemma_=='marry']).most_common()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Keywords using noun chunks from the news article (file 'article.txt').\n",
    "# Spacy will pick some noun chunks that are not informative at all (e.g. we, what, who).\n",
    "# Try to find a way to remove that kind of keywords.\n",
    "\n",
    "article = read_file('data/article.txt')\n",
    "doc = nlp(article)\n",
    "\n",
    "keywords = Counter()\n",
    "for chunk in doc.noun_chunks:\n",
    "    if nlp.vocab[chunk.lemma_].prob < - 8: # probablity value -8 is arbitrarily selected threshold\n",
    "        keywords[chunk.lemma_] += 1\n",
    "\n",
    "keywords.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Open task on the RAND Terrorism Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have preprocessed the [RAND Terrorism Dataset](http://www.rand.org/nsrd/projects/terrorism-incidents.html) for this task. \n",
    "\n",
    "Can you find out the following using the code you have written?\n",
    "\n",
    "- Who are the persons mentioned in each article?\n",
    "- What locations are mentioned in each article? Hint: a location just has a different label to a person\n",
    "- From all of your entities, can you find out which named entities are terrorists from the syntactic relationships?\n",
    "- With all of this information, can you plot a figure expressing the relationships between locations and terrorists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read file\n",
    "terror_file = read_file('data/rand-terrorism-dataset.txt')\n",
    "terror_doc = nlp(terror_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get sentences\n",
    "sentences = [s for s in terror_doc.sents]\n",
    "\n",
    "print(sentences[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find persons and organisations mentioned in the text\n",
    "\n",
    "persons = Counter()\n",
    "for ent in terror_doc.ents:\n",
    "    if ent.label_ == 'PERSON' or  ent.label_ == 'ORG':\n",
    "        persons[ent.lemma_] += 1\n",
    "print(persons.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find locations mentioned in the text\n",
    "\n",
    "locs = Counter()\n",
    "for ent in terror_doc.ents:\n",
    "    if ent.label_ == 'GPE':\n",
    "        locs[ent.lemma_] += 1\n",
    "print(locs.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You might think that in order to extract the terrorists from the text and ignore the civilians,\n",
    "#we could compare the word similariy with \"terrorism\" however below shows that results with this\n",
    "#approach have a very low recall\n",
    "\n",
    "terrorism_str = nlp('terrorists')[0]\n",
    "terrorists = Counter()\n",
    "for person in persons:\n",
    "    person = nlp(person)[0]\n",
    "    if person.similarity(terrorism_str) > 0.25:\n",
    "        terrorists[person]+=1\n",
    "print(terrorists.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#An alternative approach (inspired by http://spacy.io/docs/tutorials/twitter-filter) is shown\n",
    "#here. However if you run this you will see all sentences are classified as positive\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def classify_sentence(sentence):\n",
    "    def get_vector(word):\n",
    "        return nlp.vocab[word].repvec\n",
    "    \n",
    "    def cos(v1,v2):\n",
    "        return dot(v1,v2) / (norm(v1)*norm(v2))\n",
    "  \n",
    "    sen = [w.repvec for w in sentence]\n",
    "    accept = map(get_vector,'Terrorists'.split())\n",
    "    reject = map(get_vector,'Civilians'.split())\n",
    "    \n",
    "    y = sum(max(cos(w1,w2),0) for w1 in sen for w2 in accept)\n",
    "    n = sum(max(cos(w1,w2),0) for w1 in sen for w2 in reject)\n",
    "    \n",
    "    if (y/(y+n)) >= 0.5 or True:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run sentence through classifier and print how many (of 30367) pass positive\n",
    "\n",
    "count = 0\n",
    "for sen in sentences:\n",
    "    if classify_sentence(sen) is True:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have shown two ways to distinguish terrorists from other persons in the text. Neither were successful. Maybe you can find a better way by analysing the syntactic dependencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the number of terrorist incidents in a country over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve all the locations and store in a defaultdict object\n",
    "locations = defaultdict(list)\n",
    "for ent in terror_doc.ents:\n",
    "    if ent.label_ == 'GPE':\n",
    "        locations[ent.lemma_].append(ent.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the same procedure as for Pride and Prejudice we can extract the occurrences of a place over time.\n",
    "# For simplicity, commonly mentioned places inside a country and have been added to the country count\n",
    "\n",
    "NUM_BINS = 40\n",
    "\n",
    "iraq_occurrences = normalize_occurences(locations['iraq']) \\\n",
    "    + normalize_occurences(locations['baghdad'])\\\n",
    "    + normalize_occurences(locations['kirkuk'])\\\n",
    "    + normalize_occurences(locations['mosul']) \\\n",
    "    + normalize_occurences(locations['baqubah']) \n",
    "\n",
    "thai_occurrences = normalize_occurences(locations['thailand']) \\\n",
    "    + normalize_occurences(locations['bangkok']) \\\n",
    "    + normalize_occurences(locations['narathiwat']) \\\n",
    "    + normalize_occurences(locations['pattani'])\n",
    "\n",
    "us_occurrences = normalize_occurences(locations['us'])\\\n",
    "    + normalize_occurences(locations['u.s.'])\n",
    "    \n",
    "israel_occurrences = normalize_occurences(locations['israel']) \\\n",
    "    + normalize_occurences(locations['tel aviv']) \\\n",
    "    + normalize_occurences(locations['jerusalem'])\n",
    "    \n",
    "palestine_occurrences = normalize_occurences(locations['palestine']) \\\n",
    "    + normalize_occurences(locations['gaza'])\n",
    "\n",
    "afg_occurrences = normalize_occurences(locations['afghanistan']) \\\n",
    "    + normalize_occurences(locations['kabul'])\n",
    "\n",
    "lebanon_occurrences = normalize_occurences(locations['lebanon']) \\\n",
    "    + normalize_occurences(locations['beirut'])\n",
    "    \n",
    "turkey_occurrences = normalize_occurences(locations['turkey']) \\\n",
    "    + normalize_occurences(locations['istanbul']) \\\n",
    "    + normalize_occurences(locations['constantinople']) \n",
    "\n",
    "#Create labels and data list\n",
    "label=['Iraq','Thailand','USA','Israel','Palestine','Afghanistan','Lebanon','Turkey']\n",
    "x = [iraq_occurrences,\n",
    "     thai_occurrences,\n",
    "     us_occurrences,\n",
    "     israel_occurrences,\n",
    "     palestine_occurrences,\n",
    "     afg_occurrences,\n",
    "     lebanon_occurrences,\n",
    "     turkey_occurrences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It's now possible to plot the a histogram and line graph of incidents by country over time\n",
    "#I've added additional features to change the default colour cycle of the plot\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(x, NUM_BINS, histtype='bar', label=label)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2, borderaxespad=0.)\n",
    "matplotlib.rcParams['axes.prop_cycle'] = mpl.cycler(color=['r','k','c','b','y','m','g','#54a1FF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A line graph is easier to interpret so transform the previous plot\n",
    "\n",
    "plt.figure()\n",
    "for a in n:\n",
    "    x_val = [x/(NUM_BINS-1) for x in range(len(a))]\n",
    "    plt.plot(x_val,a)\n",
    "plt.legend(label,bbox_to_anchor=(1,1),loc=2)\n",
    "matplotlib.rcParams['axes.prop_cycle'] = mpl.cycler(color=['r','k','c','b','y','m','g','#54a1FF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a relationship between terrorist group and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In order to determine which terrorists have been mentioned in conjuction with each country\n",
    "#we need to build a new dictionary storing the number of mentions in an article of a country\n",
    "#for each terrorist group we want to look at\n",
    "\n",
    "#Specify the locations and terrorist groups we want to inspect\n",
    "common_terr = ['taliban','al - qaeda','hamas','fatah','plo','bilad al - rafidayn']\n",
    "common_locs = ['iraq','baghdad','kirkuk','mosul','afghanistan','kabul','basra','palestine','gaza','israel','istanbul','beirut','pakistan']\n",
    "location_entity_dict = defaultdict(Counter)\n",
    "\n",
    "with open('data/rand-terrorism-dataset.txt') as f:\n",
    "    for line in f: #Read the file line by line\n",
    "        article = nlp(line) #Run spaCy NLP extraction\n",
    "        \n",
    "        #Get all the groups and locations in the article\n",
    "        article_ents = [ent.lemma_ for ent in article.ents if ent.label_ == 'PERSON' or ent.label_ == 'ORG']\n",
    "        article_locs = [ent.lemma_ for ent in article.ents if ent.label_ =='GPE']\n",
    "\n",
    "        #Filter groups and locations for only those which we are interested in\n",
    "        ents_filtered = [ent for ent in article_ents if ent in common_terr]\n",
    "        locs_filtered = [loc for loc in article_locs if loc in common_locs]\n",
    "        \n",
    "        #For each found entity and location, increment the dictionary entry\n",
    "        for found_entity in ents_filtered:\n",
    "            for found_location in locs_filtered:\n",
    "                location_entity_dict[found_entity][found_location]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the dictionary into a pandas DataFrame and fill NaN values with zeroes\n",
    "df = pd.DataFrame.from_dict(dict(location_entity_dict),dtype=int)\n",
    "df = df.fillna(value=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Seaborn can transform a DataFrame directly into a figure\n",
    "plt.figure()\n",
    "map = sns.heatmap(df,annot=True,fmt='d',cmap='YlGnBu',cbar=False)\n",
    "plt.title('Global Incidents by Terrorist group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Example output\n",
    "\n",
    "![Heatmap of terrorist group and country](images/example_output.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
